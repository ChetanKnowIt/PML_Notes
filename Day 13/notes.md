# Notes:
-----------
### Topics:
1. Autoecoders
2. RNN - recurrent neural networks
3. Gated Recurrent Unit (GRU) and Long Short-Term Memory (LSTM)
4. RNN with tensorflow
5. convert pandas object to numpy with .values
6. vanishing gradient problem leads to weights not updating to lower layers
7. np.sqrt(mean_squared_error(predicted_values, test_set))
8. LSTM practical for 3 datasets 
	- AusGas
	- FRED
	- Bundesbank
9. Text Mining
	* ML models are numerical or categorical and not words or paragraphs
	* we get numerical data/categorical from text by some process
10. corpus
11. Types of text mining:
	* bag of words
	* 
12. Count vectorization
13. TDM - term document matrix
13. DTM - document term matrix is TDM^-1
14. TF - term frequency
15. IDF - inverse document frequency or log_e(Total no of doc/number of documents with term t in it)
	* rarer the word, higher the IDF
16. TF-IDF - product of TF and IDF for a term




### Links:
1. https://www.jeremyjordan.me/autoencoders/
2. https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks
3. https://github.com/ChetanKnowIt/stanford-cs-230-deep-learning
4. https://blog.mlreview.com/understanding-lstm-and-its-diagrams-37e2f46f1714
5. https://wrosinski.github.io/fe_categorical_encoding/.
6. https://www.nltk.org/book/ch01.html

 